services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - mode: ingress
        target: 8000
    environment:
      # Database Configuration
      DATABASE_URL:
      # JWT Configuration (legacy, kept for backwards compatibility)
      JWT_SECRET:
      JWT_EXPIRES_MINUTES: "10080"

      # Clerk Authentication (required for production)
      CLERK_SECRET_KEY: ${CLERK_SECRET_KEY}
      CLERK_PUBLISHABLE_KEY: ${CLERK_PUBLISHABLE_KEY}
      CLERK_WEBHOOK_SECRET: ${CLERK_WEBHOOK_SECRET}

      # Anthropic API Key
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      
      # OpenAI API Key
      OPENAI_API_KEY: 

      # Token Encryption (Required)
      TOKEN_ENCRYPTION_KEY:

      # Meta OAuth Configuration
      META_APP_ID:
      META_APP_SECRET:
      META_OAUTH_SCOPES: ${META_OAUTH_SCOPES:-}
      META_OAUTH_CONFIG_ID: ${META_OAUTH_CONFIG_ID:-} 

      # Google OAuth Configuration
      GOOGLE_DEVELOPER_TOKEN: ${GOOGLE_DEVELOPER_TOKEN:-}
      GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID:-}
      GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET:-}

      # Shopify OAuth Configuration
      SHOPIFY_API_KEY: ${SHOPIFY_API_KEY:-}
      SHOPIFY_API_SECRET: ${SHOPIFY_API_SECRET:-}

       # OAuth Redirect URIs
      META_OAUTH_REDIRECT_URI:
      GOOGLE_REDIRECT_URI: "https://backend--8000.beta.adnavi.t8zgrthold5r2.defang.app/auth/google/callback"
      SHOPIFY_OAUTH_REDIRECT_URI: "https://backend--8000.beta.adnavi.t8zgrthold5r2.defang.app/auth/shopify/callback"
      FRONTEND_URL: "https://www.metricx.ai"
      BACKEND_URL: "https://backend--8000.beta.adnavi.t8zgrthold5r2.defang.app"

      # Forwarded Allow IPs
      FORWARDED_ALLOW_IPS: "*"
      
      # Redis Configuration
      # Railway Redis URL configured via environment variable
      REDIS_URL: ${REDIS_URL:-redis://localhost:6379/0}
      
      # CORS Configuration - allow production frontend, localhost, and defang domains
      BACKEND_CORS_ORIGINS: "https://www.metricx.ai,http://localhost:3000,https://t8zgrthold5r2-backend--8000.prod2.defang.dev,https://backend--8000.beta.adnavi.t8zgrthold5r2.defang.app"

      # ===========================================
      # Observability Stack
      # ===========================================

      # Environment (production, staging, development)
      ENVIRONMENT: ${ENVIRONMENT:-production}
      LOG_LEVEL: ${LOG_LEVEL:-WARNING}

      # Sentry - Error Tracking
      SENTRY_DSN: ${SENTRY_DSN:-}

      # RudderStack - User Analytics â†’ GA4
      RUDDERSTACK_WRITE_KEY: ${RUDDERSTACK_WRITE_KEY:-}
      RUDDERSTACK_DATA_PLANE_URL: ${RUDDERSTACK_DATA_PLANE_URL:-}

      # Langfuse - LLM Observability
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-}
      LANGFUSE_HOST: ${LANGFUSE_HOST:-https://cloud.langfuse.com}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s
    deploy:
      resources:
        reservations:
          # Increased from 512M to 1024M to handle concurrent users
          # WHY: App was crashing with multiple users due to OOM
          # REF: docs/PERFORMANCE_INVESTIGATION.md
          memory: 1024M

  # RQ Worker - processes sync jobs AND QA jobs from Redis queue
  worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
    command: python -m app.workers.start_arq_worker
    environment:
      # Required for worker
      DATABASE_URL: ${DATABASE_URL}
      REDIS_URL: ${REDIS_URL:-redis://localhost:6379/0}
      TOKEN_ENCRYPTION_KEY: ${TOKEN_ENCRYPTION_KEY}
      JWT_SECRET: ${JWT_SECRET}

      # Required for QA processing
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}

      # Google SDK config (required for SDK, not user tokens)
      GOOGLE_DEVELOPER_TOKEN: ${GOOGLE_DEVELOPER_TOKEN:-}
      GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID:-}
      GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET:-}

      # Optional: Only used for /from-env fallback endpoints (OAuth provides these)
      META_ACCESS_TOKEN: ${META_ACCESS_TOKEN:-}
      META_AD_ACCOUNT_ID: ${META_AD_ACCOUNT_ID:-}
      GOOGLE_REFRESH_TOKEN: ${GOOGLE_REFRESH_TOKEN:-}
      GOOGLE_CUSTOMER_ID: ${GOOGLE_CUSTOMER_ID:-}

      # Observability (worker also processes LLM calls)
      ENVIRONMENT: ${ENVIRONMENT:-production}
      SENTRY_DSN: ${SENTRY_DSN:-}
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-}
      LANGFUSE_HOST: ${LANGFUSE_HOST:-https://cloud.langfuse.com}
    deploy:
      resources:
        reservations:
          # Increased from 256M to 512M for Claude API processing
          # WHY: Workers handle LLM calls which have larger memory footprint
          memory: 512M
    restart: always

  # Scheduler - enqueues sync jobs based on connection frequency settings
  scheduler:
    build:
      context: ./backend
      dockerfile: Dockerfile.scheduler
    environment:
      DATABASE_URL: ${DATABASE_URL}
      REDIS_URL: ${REDIS_URL:-redis://localhost:6379/0}
      # Error tracking
      ENVIRONMENT: ${ENVIRONMENT:-production}
      SENTRY_DSN: ${SENTRY_DSN:-}
    deploy:
      resources:
        reservations:
          memory: 128M
    restart: always
